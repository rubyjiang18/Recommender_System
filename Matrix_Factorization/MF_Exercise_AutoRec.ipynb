{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoRec\n",
    "1. User the **preprocess2sparse.py** to format the data for autorec\n",
    "2. Use this **autorec.py**, i.e., this script to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "reg = 0.0001\n",
    "# reg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = load_npz(\"Atrain.npz\")\n",
    "A_test = load_npz(\"Atest.npz\")\n",
    "mask = (A > 0) * 1.0\n",
    "mask_test = (A_test > 0) * 1.0\n",
    "\n",
    "# make copies since we will shuffle\n",
    "# each row index is user id\n",
    "A_copy = A.copy()\n",
    "mask_copy = mask.copy()\n",
    "A_test_copy = A_test.copy()\n",
    "mask_test_copy = mask_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 138493 M: 26744\n",
      "N // batch_size: 1081\n",
      "mu: 3.5255907266217132\n"
     ]
    }
   ],
   "source": [
    "N, M = A.shape\n",
    "print(\"N:\", N, \"M:\", M)\n",
    "print(\"N // batch_size:\", N // batch_size)\n",
    "\n",
    "# center the data\n",
    "mu = A.sum() / mask.sum()\n",
    "print(\"mu:\", mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rubyjiang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubyjiang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:573: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  append_fn(tensor_proto, proto_values)\n",
      "/Users/rubyjiang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:538: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    }
   ],
   "source": [
    "# build the model - just a 1 hidden layer autoencoder\n",
    "i = Input(shape=(M,))\n",
    "# bigger hidden layer size seems to help!\n",
    "x = Dropout(0.7)(i) # drop out is very important! cannot miss this.\n",
    "x = Dense(700, activation='tanh', kernel_regularizer=l2(reg))(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(M, kernel_regularizer=l2(reg))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot use the keras build in loss\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mask = K.cast(K.not_equal(y_true, 0), dtype='float32')\n",
    "    diff = y_pred - y_true\n",
    "    sqdiff = diff * diff * mask\n",
    "    sse = K.sum(K.sum(sqdiff))\n",
    "    n = K.sum(K.sum(mask))\n",
    "    return sse / n\n",
    "\n",
    "# data generator, specifically for training\n",
    "def generator(A, M):\n",
    "    while True:\n",
    "        A, M = shuffle(A, M)\n",
    "        for i in range(A.shape[0] // batch_size + 1):\n",
    "            # the size of last batch < the batch_size\n",
    "            upper = min((i+1)*batch_size, A.shape[0])\n",
    "            \n",
    "            a = A[i*batch_size:upper].toarray()\n",
    "            m = M[i*batch_size:upper].toarray()\n",
    "            a = a - mu * m # must keep zeros at zero!\n",
    "            # m2 = (np.random.random(a.shape) > 0.5)\n",
    "            # noisy = a * m2\n",
    "            noisy = a # no noise\n",
    "            yield noisy, a # return inputs and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the test generator.\n",
    "\n",
    "As you can see, it takes in both the training data and the test data.\n",
    "\n",
    "This is important since, if you recall, the training data is what we need to predict the test data.\n",
    "\n",
    "You don't want the auto encoder to literally work like an auto encoder and use the test data to predict the test data. That's what a normal auto encoder does.\n",
    "\n",
    "But that's not what we want to do, because the purpose of a recommender system is that we want to predict ratings that we haven't seen yet.\n",
    "\n",
    "So in this loop, we **don't need to shuffle**.\n",
    "\n",
    "We just grab the current batch that's little, a little M, little a T and little M for the test data. So the input is just a minus mu times M as usual, but the target is now a t minus mu times m t, which comes from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test gen takes both train and test data            \n",
    "def test_generator(A, M, A_test, M_test):\n",
    "  # assumes A and A_test are in corresponding order\n",
    "  # both of size N x M\n",
    "    while True:\n",
    "        for i in range(A.shape[0] // batch_size + 1):\n",
    "            upper = min((i+1)*batch_size, A.shape[0])\n",
    "            a = A[i*batch_size:upper].toarray()\n",
    "            m = M[i*batch_size:upper].toarray()\n",
    "            at = A_test[i*batch_size:upper].toarray()\n",
    "            mt = M_test[i*batch_size:upper].toarray()\n",
    "            a = a - mu * m\n",
    "            at = at - mu * mt\n",
    "            yield a, at # a is input, at is target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(i, x)\n",
    "model.compile(\n",
    "  loss=custom_loss,\n",
    "  optimizer=SGD(lr=0.08, momentum=0.9),\n",
    "  # optimizer='adam',\n",
    "  metrics=[custom_loss],\n",
    ")\n",
    "\n",
    "\n",
    "r = model.fit(\n",
    "  generator(A, mask),\n",
    "  validation_data=test_generator(A_copy, mask_copy, A_test_copy, mask_test_copy),\n",
    "  epochs=epochs,\n",
    "  steps_per_epoch=A.shape[0] // batch_size + 1,\n",
    "  validation_steps=A_test.shape[0] // batch_size + 1,\n",
    ")\n",
    "print(r.history.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(r.history['loss'], label=\"train loss\")\n",
    "plt.plot(r.history['val_loss'], label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot mse\n",
    "plt.plot(r.history['custom_loss'], label=\"train mse\")\n",
    "plt.plot(r.history['val_custom_loss'], label=\"test mse\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
