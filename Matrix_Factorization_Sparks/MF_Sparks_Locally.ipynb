{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "<img src=\"intro1.png\" alt=\"score\" width=\"500\" height=\"200\" \n",
    "style=\"float:left;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the key technology for handling datasets that are this big is called big data. Unfortunately, these days, big data is a buzzword and everyone's got their own little definition. **I like to think of big data technology as any technology that requires distributed computing.** \n",
    "\n",
    "Case 1: if you have a database, but that database uses what's called sharding, that means user IDs, 1 to 1000 will be on one machine, whereas user IDs, 1001 to 2000 will be on another machine and so on.\n",
    "\n",
    "Case 2: You can even sharding the files themselves. So if you have a humongous log file of all the events that are happening on your website and say the file is one terabyte large, then you might store pieces of that file across different machines around the world.\n",
    "\n",
    "Case 3: Finally, you can distribute compute itself. So imagine you have to do a very long for loop, just like we have to do in matrix factorization. So we have to write for i in range N but N is 1.8 billion. So that's not going to happen on a single machine in any reasonable amount of time. Instead, the first 1000 users will be processed on one machine. The second 1000 users will be processed on another machine and so forth. This is what we're mostly interested in, at least for this course. Our actual data file is small enough to fit on our local machine, but the algorithm that we use to process it in particular matrix factorization, can be sped up by distributing the compute across multiple machines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"intro2.png\" alt=\"score\" width=\"500\" height=\"200\" style=\"float:left;\">\n",
    "<img src=\"intro3.png\" alt=\"score\" width=\"400\" height=\"200\" style=\"float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Outline\n",
    "<img src=\"intro4.png\" alt=\"score\" width=\"400\" height=\"200\" style=\"float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spark install on local MacOS\n",
    "Detailed steps are not included here.\n",
    "\n",
    "After install, termal \"spark-shell\", test it with some code, and \":q\" to exit.\n",
    "\n",
    "What we'll actually be using is pyspark. It's still spark, but we can write our code in Python. This is the interface we'll be using for writing our matrix factorization code. So if you've gotten this far, then you're ready to head over to that lecture. In terminal, type \"pyspark\".\n",
    "\n",
    "Basically, you need to install:\n",
    "- Homebrew\n",
    "- Xcode\n",
    "- Java\n",
    "- Scala\n",
    "- apache-spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MF in Spark on local machine\n",
    "## 3.1 \n",
    "Meant to be pasted into console, not in jupyter notebook.\n",
    "\n",
    "Check the **spark.py** script.\n",
    "\n",
    "To run it, initiate the \"pyspark\" command in terminal, then past the code to terminal.\n",
    "\n",
    "## 3.2 \n",
    "A way to run a python script on Spark.\n",
    "\n",
    "Check the **spark2.py** script.\n",
    "\n",
    "This is, by the way, a stepping stone for the next step, which is running the job on a cluster using Amazon Web services.\n",
    "\n",
    "To run the code, in terminal :\n",
    "- do not initiate the pyspark command\n",
    "- spark-submit --master spark://localhost:7077 spark2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MF in Spark on AWS/EC2\n",
    "\n",
    "Check out the video: https://www.udemy.com/course/recommender-systems/learn/lecture/11798760#overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Use Spark to make predictions in the real world\n",
    "\n",
    "<img src=\"intro5.png\" alt=\"score\" width=\"500\" height=\"200\" style=\"float:left;\">\n",
    "\n",
    "<img src=\"intro6.png\" alt=\"score\" width=\"500\" height=\"200\" style=\"float:left;\">\n",
    "\n",
    "So there's a discrepancy between the timescales of these two kinds of tasks.\n",
    "\n",
    "Most likely your API is pulling data from some database, maybe Postgres or MySQL. So then your job in the Spark script is to save your output to this database. We won't discuss the code to do that, but you shouldn't have too much trouble finding a one liner through Google.\n",
    "\n",
    "<img src=\"intro7.png\" alt=\"score\" width=\"500\" height=\"200\" style=\"float:left;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
