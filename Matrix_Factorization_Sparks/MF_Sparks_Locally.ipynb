{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "<img src=\"intro1.png\" alt=\"score\" width=\"500\" height=\"200\" \n",
    "style=\"float:left;\">\n",
    "\n",
    "So the key technology for handling datasets that are this big is called big data. Unfortunately, these days, big data is a buzzword and everyone's got their own little definition. **I like to think of big data technology as any technology that requires distributed computing.** \n",
    "\n",
    "Case 1: if you have a database, but that database uses what's called sharding, that means user IDs, 1 to 1000 will be on one machine, whereas user IDs, 1001 to 2000 will be on another machine and so on.\n",
    "\n",
    "Case 2: You can even sharding the files themselves. So if you have a humongous log file of all the events that are happening on your website and say the file is one terabyte large, then you might store pieces of that file across different machines around the world.\n",
    "\n",
    "Case 3: Finally, you can distribute compute itself. So imagine you have to do a very long for loop, just like we have to do in matrix factorization. So we have to write for i in range N but N is 1.8 billion. So that's not going to happen on a single machine in any reasonable amount of time. Instead, the first 1000 users will be processed on one machine. The second 1000 users will be processed on another machine and so forth. This is what we're mostly interested in, at least for this course. Our actual data file is small enough to fit on our local machine, but the algorithm that we use to process it in particular matrix factorization, can be sped up by distributing the compute across multiple machines.\n",
    "\n",
    "<img src=\"intro2.png\" alt=\"score\" width=\"500\" height=\"200\" style=\"float:left;\">\n",
    "<img src=\"intro3.png\" alt=\"score\" width=\"400\" height=\"200\" style=\"float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Outline\n",
    "<img src=\"intro4.png\" alt=\"score\" width=\"400\" height=\"200\" style=\"float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spark install on local MacOS\n",
    "Detailed steps are not included here.\n",
    "\n",
    "After install, termal \"spark-shell\", test it with some code, and \":q\" to exit.\n",
    "\n",
    "What we'll actually be using is pyspark. It's still spark, but we can write our code in Python. This is the interface we'll be using for writing our matrix factorization code. So if you've gotten this far, then you're ready to head over to that lecture. In terminal, type \"pyspark\".\n",
    "\n",
    "Basically, you need to install:\n",
    "- Homebrew\n",
    "- Xcode\n",
    "- Java\n",
    "- Scala\n",
    "- apache-spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MF in Spark on local machine\n",
    "### meant to be pasted into console, not in jupyter notebook\n",
    "Check the **spark.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes:\n",
    "# you may have trouble with full dataset on just your local machine\n",
    "# if you want to know what's in an RDD, use .take(n), ex:\n",
    "# tmp = p.take(5)\n",
    "# print(tmp)\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "import os\n",
    "\n",
    "# load in the data\n",
    "data = sc.textFile(\"../large_files/movielens-20m-dataset/small_rating.csv\")\n",
    "\n",
    "# filter out header\n",
    "header = data.first() #extract header\n",
    "data = data.filter(lambda row: row != header)\n",
    "\n",
    "# convert into a sequence of Rating objects\n",
    "ratings = data.map(\n",
    "  lambda l: l.split(',')\n",
    ").map(\n",
    "  lambda l: Rating(int(l[0]), int(l[1]), float(l[2]))\n",
    ")\n",
    "\n",
    "# split into train and test\n",
    "train, test = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# train the model\n",
    "K = 10\n",
    "epochs = 10\n",
    "model = ALS.train(train, K, epochs)\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "# train\n",
    "x = train.map(lambda p: (p[0], p[1]))\n",
    "p = model.predictAll(x).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = train.map(lambda r: ((r[0], r[1]), r[2])).join(p)\n",
    "# joins on first item: (user_id, movie_id)\n",
    "# each row of result is: ((user_id, movie_id), (rating, prediction))\n",
    "mse = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"train mse: %s\" % mse)\n",
    "\n",
    "\n",
    "# test\n",
    "x = test.map(lambda p: (p[0], p[1]))\n",
    "p = model.predictAll(x).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(p)\n",
    "mse = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"test mse: %s\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
