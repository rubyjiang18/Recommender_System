{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section outline\n",
    "1. [Popularity](#popularity)\n",
    "2. [Recency](#recency)\n",
    "3. [Problem with Average; How confident are we in the average](#average)\n",
    "4. [Bayesian Ranking](#bayesian)\n",
    "5. [Supervised ML for Recommendation](#sml)\n",
    "6. [Markov Model and PageRank](#pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='popularity'>1. Basic Intuition </a>\n",
    "If you had to get your recommender system up and running within the next few days. Most likely you would opt for something very simple.\n",
    "\n",
    "<img src=\"images/intro1.png\" alt=\"score\" width=\"600\" height=\"200\">\n",
    "\n",
    "<img src=\"images/intro2.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro3.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "Another basic intuition is **Product Association**, things usually bought together. This is an example using context to guide recommendations.\n",
    "<img src=\"images/intro4.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro5.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro6.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro7.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='recency'>2.1 Hacker News </a>\n",
    "### Key idea is how to balance f(popularity) / f(age)\n",
    "\n",
    "<img src=\"images/intro8.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro9.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro10.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro11.png\" alt=\"score\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/intro12.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "What this blogger did was track the rankings of articles for some time to get back some nice plots and to determine whether or not Hacker News actually uses the formula that it's published.\n",
    "\n",
    "What's cool about this is you can see exactly how an article score evolves over time. So a typical pattern is the score will grow very quickly and then taper off at a slower rate due to the gravity term.\n",
    "\n",
    "The reason it's able to grow so quickly is because an article will receive its votes very early on, and gravity doesn't have much of an effect during that time. But after a while, the gravity term starts to take over.\n",
    "\n",
    "What's also interesting about this post is that you'll learn that the penalty term isn't a known formula.\n",
    "\n",
    "The author tries to guess based on the data he collected, and one pattern that he noticed was that a lot of things seemed to get penalized. \n",
    "- You can get penalized for being a link to a site that's very popular, such as GitHub.com or Reddit.com. \n",
    "- You can get penalized if your article is about the NSA, although it says in the Post that this is no longer the case.\n",
    "- You can even get penalized if your link receives too many comments, which is used as a proxy for how controversial the link is.\n",
    "\n",
    "What's interesting is that since we don't actually know the exact penalties, it's possible that penalties could be applied for any reason or no reason at all. One person commented on this blog post reminding us that there is a conflict of interest there as well. Since Hacker News is owned by Y Combinator, then links about Y Combinator companies could be unfairly pushed to the top, while links about other companies could be unfairly pushed down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Raddit\n",
    "<img src=\"images/intro13.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro14.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "Notice that age = time the post is posted - t0, such that newer post has larger age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='average'>3. Problems with Average Rating & Explore vs. Exploit </a>\n",
    "Intro contents are not included here as I know it well. \n",
    "\n",
    "<img src=\"images/intro15.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "What if X is not noramlly distrbuted?\n",
    "- No problem, CLT, X_bar is approx normally distributed.\n",
    "\n",
    "<img src=\"images/intro16.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "\n",
    "A better estimation than Bernoulli, is the following\n",
    "<img src=\"images/intro17.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro18.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro19.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "<img src=\"images/intro20.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro21.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro22.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "### Solution? Baysian Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='bayesian'>4.Bayesian Ranking </a>\n",
    "\n",
    "Idea: what if we sort by **random** number, rather than fix number like average and lower bound?\n",
    "- Random means a random variable is characterized by its characterization, not disorder\n",
    "\n",
    "<img src=\"images/intro23.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro24.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "How do we learn these distributions? These distributions will start out completely flat, meaning we assume nothing about their CTR. However, as time goes on and people visit our website, we're going to collect data. Every time we collect the data point, we're going to update these distributions. So for example, every time a user sees a product that will count as a view, every time a user clicks on a product that will count as a click.\n",
    "\n",
    "<img src=\"images/intro25.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "1. Wrong anser: we do not want to sort by the mean of each distribution.\n",
    "2. Solution: ***Thompson Sampling***: Sample random numbers.\n",
    "<img src=\"images/intro26.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "#### Extreme Cases\n",
    "It's helpful to think of some extreme examples. \n",
    "\n",
    "**Case 1**: Consider the case where both items are uniform.\n",
    "\n",
    "In this case, the samples we draw are completely random and it's not really predictable, which would come first. This is perfectly OK because remember that we're still at the stage where we want to explore and collect more data. If you don't know anything about which of these items are more likely to be clicked on, then we need to collect data to give us a better idea in the real world.\n",
    "\n",
    "This translates to the idea that we need to actually show these items to users to see if they will be clicked, or not only after users have seen the items will we have any idea of how good they are.\n",
    "\n",
    "**Case 2**:Now, consider the case where both items have very sharp peaks. In this case, when we draw samples from these distributions, the samples are going to be very close to the peak. As an example, consider the peak at two thirds, we might get a sample of 0.6665 or 0.6668. But there's essentially no chance we're going to draw a sample like, say, 0.2. Because of this, it's highly likely that the item whose peak is located at the highest position will win to connect this back to the real world. This is the situation where we've already collected millions of data points for both items because we've collected so much data.\n",
    "\n",
    "**Case 3**: Now, let's consider a mixed scenario where we have one item with a very sharp peak and one item with a very fat peak. In this case, the simple for the sharp peak will most likely be located at the peak. On the other hand, the item with the very fat peak could really have a sample anywhere. As such, there is a good chance it may be ranked higher than the other item. But there's also a good chance it maybe ranks lower. And remember, many people are going to see this page, so there's going to be a mix of both. So sometimes item one will be ranked higher than Item two and sometimes vice versa. And remember that this is good so that we can collect more data. You can think of the fat item as being an exploration mode because we're still not very confident of its click through rate.\n",
    "\n",
    "So to summarize what we have just learned, we've learned that the Bayesian method automatically balances the need to explore and the need to exploit in scenario one. We saw that it handles the need to explore by essentially sorting items in a completely random fashion in scenario two. We saw that it handles the need to exploit by essentially sorting items deterministic leave by their known stars.\n",
    "\n",
    "In scenario three, we saw that these two extremes can coexist, where you can have one item which needsmore exploration because it needs more data. And the one item which can be exploited because it already has enough data. In other words, this solves the Explore exploit dilemma. So the nice thing about this method is that it is completely automatic. You don't need to run an a b test to figure out which item is best. You can just run this algorithm and everything is ranked automatically in an optimal way.\n",
    "\n",
    "Check out the Bandit Demo in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='sml'>5. Demographics and Supervised Learning </a>\n",
    "<img src=\"images/intro27.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro28.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "#### What about the item itself?\n",
    "One way is to build a separate model for each product. For example, I have one predictive model for iPhones and another predictive model for cat food. Of course, this might be problematic because then you would need a lot of data per product. If I have, let's say, 10,000 products in my store, then collecting enough data to build a separate model for each product may be difficult.\n",
    "\n",
    "So what you can do is you can also include attributes of the product and build just one model, making both the user attributes and product attributes combined into one feature vector. So, for example, my feature vector might contain attributes about the user such as age, gender and location. And then after that, some attributes about the product, such as is it electronic, does it have a retina display, how much ram does it have and so forth.\n",
    "\n",
    "By doing this, I can have just a single machine learning model that takes in input features about the user and the product. We can then use a machine learning model to do classification in the case where we want to predict whether or not the user will buy the product, or we could do a regression model, in which case we would want to predict the user's rating.\n",
    "\n",
    "<img src=\"images/intro29.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "\n",
    "Now, of course, getting data like this is not an easy task in this day and age. Privacy is an important aspect of life on the Internet.\n",
    "<img src=\"images/intro30.png\" alt=\"score\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='pagerank'>6. Page Rank</a>\n",
    "\n",
    "<img src=\"images/intro31.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro32.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro33.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro34.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro35.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro36.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro37.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro38.png\" alt=\"score\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Rank Part 2\n",
    "\n",
    "$\\pi_t$ : Stateprobability  distribution at time t (probability of being in a state at a particular time).\n",
    "\n",
    "Example: $\\pi_t = [\\pi_t(sunny), \\pi_t(rainy)]$ , i.e., a vector.\n",
    "\n",
    "<img src=\"images/intro39.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro40.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro41.png\" alt=\"score\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the eigenvalue problem is where, given a matrix, we're trying to find a vector and a number such that multiplying the matrix by the vector is equal to multiplying the vector by that number.\n",
    "\n",
    "Therefore, the eigenvectors are special vectors whose directions are not changed by the given matrix. And so we recognize the equation for the state distribution at time infinity as just the eigenvalue problem for which the eigenvalue is one. Again, note that the state distribution appears to the left of the matrix because it's a row vector rather than a column vector, which is the usual linear algebra convention.\n",
    "\n",
    "And finally, the key lesson on this page is if I want to know the state distribution at Time Infinity, I don't actually have to multiply a infinity times since that would be infeasible. Instead, I can just solve for the eigenvectors and eigenvalues of A.\n",
    "\n",
    "<img src=\"images/intro42.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "The raw transition probability:\n",
    "<img src=\"images/intro43.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "\n",
    "So instead of just using the raw transition probabilities, we're instead going to scale those back by 0.85 and add 0.15 times u, where u is a matrix where all the values are equal to one over M. So you can think of that as a uniform distribution. Recall that M is the total number of states. So this is also a valid probability distribution. And since 0.85 and 0.15 also add up to one. Then by adding these two weighted matrices together, I get back yet another valid probability distribution.\n",
    "<img src=\"images/intro44.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "We call it the Google Matrix G.\n",
    "\n",
    "Of course, at this point, the only step we haven't done is find the limiting distribution. So let's find the limiting distribution of G. This gives us a vector of probabilities, one for each web page on the internet. And these probabilities are precisely the PageRank of those pages. So what is PageRank? It's just the probability of ending up on a web page. If you surf the Internet randomly for an infinite amount of time. It's a very abstract concept, and yet it works so well.\n",
    "\n",
    "<img src=\"images/intro45.png\" alt=\"score\" width=\"400\" height=\"200\">\n",
    "<img src=\"images/intro46.png\" alt=\"score\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate a Ranking\n",
    "<img src=\"images/intro47.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro48.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro49.png\" alt=\"score\" width=\"500\" height=\"200\">\n",
    "<img src=\"images/intro50.png\" alt=\"score\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/intro51.png\" alt=\"score\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
